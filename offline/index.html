<!doctype html>
<html lang="en-US">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width,initial-scale=1" />
    <meta name="generator" content="VuePress 2.0.0-rc.18" />
    <style>
      :root {
        --vp-c-bg: #fff;
      }

      [data-theme='dark'] {
        --vp-c-bg: #1b1b1f;
      }

      html,
      body {
        background-color: var(--vp-c-bg);
      }
    </style>
    <script>
      const userMode = localStorage.getItem('vuepress-color-scheme')
      const systemDarkMode =
        'matchMedia' in window
          ? window.matchMedia('(prefers-color-scheme: dark)').matches
          : false

      if (userMode === 'light') {
        document.documentElement.dataset.theme = 'light'
      } else if (userMode === 'dark' || systemDarkMode) {
        document.documentElement.dataset.theme = 'dark'
      }
    </script>
    <link rel="icon" href="/learning-ai/favicon.ico"><link rel="manifest" href="/learning-ai/manifest.webmanifest"><meta name="theme-color" content="#00A67E"><title>Offline with LM Studio</title><meta name="description" content="">
    <link rel="preload" href="/learning-ai/assets/style-DxALa4jE.css" as="style"><link rel="stylesheet" href="/learning-ai/assets/style-DxALa4jE.css">
    <link rel="modulepreload" href="/learning-ai/assets/app-CpFOj0gG.js"><link rel="modulepreload" href="/learning-ai/assets/index.html-BKkRnedg.js">
    <link rel="prefetch" href="/learning-ai/assets/index.html-CE996aio.js" as="script"><link rel="prefetch" href="/learning-ai/assets/index.html-RxFH6ihY.js" as="script"><link rel="prefetch" href="/learning-ai/assets/index.html-BI6zCFdP.js" as="script"><link rel="prefetch" href="/learning-ai/assets/index.html-DWyx6_xv.js" as="script"><link rel="prefetch" href="/learning-ai/assets/index.html-BQRe6DYZ.js" as="script"><link rel="prefetch" href="/learning-ai/assets/index.html-ePbs5NrP.js" as="script"><link rel="prefetch" href="/learning-ai/assets/404.html-OWTotRQR.js" as="script"><link rel="prefetch" href="/learning-ai/assets/setupDevtools-7MC2TMWH-BvTCuljQ.js" as="script">
  </head>
  <body>
    <div id="app"><!--[--><div class="vp-theme-container external-link-icon" vp-container><!--[--><header class="vp-navbar" vp-navbar><div class="vp-toggle-sidebar-button" title="toggle sidebar" aria-expanded="false" role="button" tabindex="0"><div class="icon" aria-hidden="true"><span></span><span></span><span></span></div></div><span><a class="route-link" href="/learning-ai/"><img class="vp-site-logo" src="/learning-ai/logo_worldline.png" alt><!----></a></span><div class="vp-navbar-items-wrapper" style=""><!--[--><!--]--><nav class="vp-navbar-items vp-hide-mobile" aria-label="site navigation"><!--[--><div class="vp-navbar-item"><a class="auto-link external-link" href="https://github.com/worldline/learning-ai" aria-label="⭐ Contribute!" rel="noopener noreferrer" target="_blank"><!---->⭐ Contribute!<!----></a></div><!--]--></nav><!--[--><!--]--><button type="button" class="vp-toggle-color-mode-button" title="toggle color mode"><svg class="light-icon" viewbox="0 0 32 32" style=""><path d="M16 12.005a4 4 0 1 1-4 4a4.005 4.005 0 0 1 4-4m0-2a6 6 0 1 0 6 6a6 6 0 0 0-6-6z" fill="currentColor"></path><path d="M5.394 6.813l1.414-1.415l3.506 3.506L8.9 10.318z" fill="currentColor"></path><path d="M2 15.005h5v2H2z" fill="currentColor"></path><path d="M5.394 25.197L8.9 21.691l1.414 1.415l-3.506 3.505z" fill="currentColor"></path><path d="M15 25.005h2v5h-2z" fill="currentColor"></path><path d="M21.687 23.106l1.414-1.415l3.506 3.506l-1.414 1.414z" fill="currentColor"></path><path d="M25 15.005h5v2h-5z" fill="currentColor"></path><path d="M21.687 8.904l3.506-3.506l1.414 1.415l-3.506 3.505z" fill="currentColor"></path><path d="M15 2.005h2v5h-2z" fill="currentColor"></path></svg><svg class="dark-icon" viewbox="0 0 32 32" style="display:none;"><path d="M13.502 5.414a15.075 15.075 0 0 0 11.594 18.194a11.113 11.113 0 0 1-7.975 3.39c-.138 0-.278.005-.418 0a11.094 11.094 0 0 1-3.2-21.584M14.98 3a1.002 1.002 0 0 0-.175.016a13.096 13.096 0 0 0 1.825 25.981c.164.006.328 0 .49 0a13.072 13.072 0 0 0 10.703-5.555a1.01 1.01 0 0 0-.783-1.565A13.08 13.08 0 0 1 15.89 4.38A1.015 1.015 0 0 0 14.98 3z" fill="currentColor"></path></svg></button><!----></div></header><!--]--><div class="vp-sidebar-mask"></div><!--[--><aside class="vp-sidebar" vp-sidebar><nav class="vp-navbar-items" aria-label="site navigation"><!--[--><div class="vp-navbar-item"><a class="auto-link external-link" href="https://github.com/worldline/learning-ai" aria-label="⭐ Contribute!" rel="noopener noreferrer" target="_blank"><!---->⭐ Contribute!<!----></a></div><!--]--></nav><!--[--><!--]--><ul class="vp-sidebar-items"><!--[--><li><a class="route-link auto-link vp-sidebar-item vp-sidebar-heading" href="/learning-ai/" aria-label="Home"><!---->Home<!----></a><!----></li><li><a class="route-link auto-link vp-sidebar-item vp-sidebar-heading" href="/learning-ai/overview/" aria-label="Let&#39;s start"><!---->Let&#39;s start<!----></a><!----></li><li><a class="route-link auto-link vp-sidebar-item vp-sidebar-heading" href="/learning-ai/use/" aria-label="Prompt with AI"><!---->Prompt with AI<!----></a><!----></li><li><a class="route-link auto-link vp-sidebar-item vp-sidebar-heading" href="/learning-ai/llm/" aria-label="Online with Librechat"><!---->Online with Librechat<!----></a><!----></li><li><a class="route-link route-link-active auto-link vp-sidebar-item vp-sidebar-heading active" href="/learning-ai/offline/" aria-label="Offline with LM Studio"><!---->Offline with LM Studio<!----></a><!----></li><li><a class="route-link auto-link vp-sidebar-item vp-sidebar-heading" href="/learning-ai/develop/" aria-label="Develop with AI"><!---->Develop with AI<!----></a><!----></li><li><a class="route-link auto-link vp-sidebar-item vp-sidebar-heading" href="/learning-ai/servicesai/" aria-label="AI for services"><!---->AI for services<!----></a><!----></li><!--]--></ul><!--[--><!--]--></aside><!--]--><!--[--><main class="vp-page"><!--[--><!--]--><div class="theme-default-content" vp-content><!--[--><!--]--><div><h1 id="offline-with-lm-studio" tabindex="-1"><a class="header-anchor" href="#offline-with-lm-studio"><span>Offline with LM Studio</span></a></h1><div class="hint-container warning"><p class="hint-container-title">Disclamer</p><p>Be careful with offline prompting models downloaded from the internet. They can contain malicious code. And also the size of the model can be very large from few Gb to few Tb.</p></div><h2 id="definitions" tabindex="-1"><a class="header-anchor" href="#definitions"><span>Definitions</span></a></h2><p>If you don&#39;t want to use the online AI providers, you can use offline prompting. This technique involves using a local LLM to generate responses to prompts. It is useful for developers who want to use a local LLM for offline prompting or for those who want to experiment with different LLMs without relying on online providers.</p><p>LM Studio is a tool that allows developers to experiment with different LLMs without relying on online providers. It provides a user-friendly interface for selecting and configuring LLMs, as well as a chat interface for interacting with the LLMs. It also includes features for fine-tuning and deploying LLMs. This technique is useful for developers who want to experiment with different LLMs.</p><h2 id="installation" tabindex="-1"><a class="header-anchor" href="#installation"><span>Installation</span></a></h2><p><img src="/learning-ai/assets/lmstudio-B6PjFtYg.png" alt="lmstudio_installation"></p><p>For installation, you can follow the instructions <a href="https://lmstudio.ai/docs/" target="_blank" rel="noopener noreferrer">here</a></p><h2 id="model-configuration" tabindex="-1"><a class="header-anchor" href="#model-configuration"><span>Model configuration</span></a></h2><p>You can configure the model you want to use in the settings tab. You can select the model you want to use and configure it according to your needs.</p><p><code>Context Length</code>: The context length is the number of tokens that will be used as context for the model. This is important because it determines how much information the model can use to generate a response. A longer context length will allow the model to generate more detailed and relevant responses, but it may also increase the computational cost of the model.</p><p><code>GPU Offload</code>: This option allows you to offload the model to a GPU if available. This can significantly speed up the generation process, especially for longer prompts or complex models.</p><p><code>CPU Threads</code>: This option allows you to specify the number of CPU threads to use for the model. This can be useful for controlling the computational resources used by the model.</p><p><code>Evaluation batch size</code>: This option allows you to specify the batch size for evaluation. This is important for evaluating the performance of the model and can affect the speed and accuracy of the generation process.</p><p><code>RoPE Frequency base</code>: This option allows you to specify the frequency base for RoPE (Range-based Output Embedding). This is important for controlling the output length of the model and can affect the quality of the generated responses.</p><p><code>RoPE Frequency scale</code>: This option allows you to specify the frequency scale for RoPE (Range-based Output Embedding). This is important for controlling the output length of the model and can affect the quality of the generated responses.</p><p><code>Keep model in memory</code>: This option allows you to keep the model in memory after the generation process is complete. This can be useful for generating multiple responses or for using the model for offline prompting.</p><p><code>Try mmap()</code> for faster loading: This option allows you to try using mmap() for faster loading of the model. This can be useful for loading large models or for generating responses quickly.</p><p><code>Seed</code>: This option allows you to specify a seed for the model. This can be useful for controlling the randomness of the generated responses.</p><p><code>Flash Attention</code>: This option allows you to enable flash attention for the model. This can be useful for generating more detailed and accurate responses, but it may also increase the computational cost of the model.</p><h2 id="enable-apis" tabindex="-1"><a class="header-anchor" href="#enable-apis"><span>enable APIs</span></a></h2><p>You can use the APIs to generate responses from the models. To enable the API server with LM Studio, you need to set the <code>API Server</code> option to <code>ON</code> in the settings tab. You can then use the API endpoints to generate responses from the models.</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line"><span class="token number">2024</span>-11-15 <span class="token number">18</span>:45:22  <span class="token punctuation">[</span>INFO<span class="token punctuation">]</span> <span class="token punctuation">[</span>LM STUDIO SERVER<span class="token punctuation">]</span> Success<span class="token operator">!</span> HTTP server listening on port <span class="token number">1234</span></span>
<span class="line"><span class="token number">2024</span>-11-15 <span class="token number">18</span>:45:22  <span class="token punctuation">[</span>INFO<span class="token punctuation">]</span></span>
<span class="line"><span class="token number">2024</span>-11-15 <span class="token number">18</span>:45:22  <span class="token punctuation">[</span>INFO<span class="token punctuation">]</span> <span class="token punctuation">[</span>LM STUDIO SERVER<span class="token punctuation">]</span> Supported endpoints:</span>
<span class="line"><span class="token number">2024</span>-11-15 <span class="token number">18</span>:45:22  <span class="token punctuation">[</span>INFO<span class="token punctuation">]</span> <span class="token punctuation">[</span>LM STUDIO SERVER<span class="token punctuation">]</span> -<span class="token operator">&gt;</span>	GET  http://localhost:1234/v1/models</span>
<span class="line"><span class="token number">2024</span>-11-15 <span class="token number">18</span>:45:22  <span class="token punctuation">[</span>INFO<span class="token punctuation">]</span> <span class="token punctuation">[</span>LM STUDIO SERVER<span class="token punctuation">]</span> -<span class="token operator">&gt;</span>	POST http://localhost:1234/v1/chat/completions</span>
<span class="line"><span class="token number">2024</span>-11-15 <span class="token number">18</span>:45:22  <span class="token punctuation">[</span>INFO<span class="token punctuation">]</span> <span class="token punctuation">[</span>LM STUDIO SERVER<span class="token punctuation">]</span> -<span class="token operator">&gt;</span>	POST http://localhost:1234/v1/completions</span>
<span class="line"><span class="token number">2024</span>-11-15 <span class="token number">18</span>:45:22  <span class="token punctuation">[</span>INFO<span class="token punctuation">]</span> <span class="token punctuation">[</span>LM STUDIO SERVER<span class="token punctuation">]</span> -<span class="token operator">&gt;</span>	POST http://localhost:1234/v1/embeddings</span>
<span class="line"><span class="token number">2024</span>-11-15 <span class="token number">18</span>:45:22  <span class="token punctuation">[</span>INFO<span class="token punctuation">]</span></span>
<span class="line"><span class="token number">2024</span>-11-15 <span class="token number">18</span>:45:22  <span class="token punctuation">[</span>INFO<span class="token punctuation">]</span> <span class="token punctuation">[</span>LM STUDIO SERVER<span class="token punctuation">]</span> Logs are saved into /Users/ibrahim/.cache/lm-studio/server-logs</span>
<span class="line"><span class="token number">2024</span>-11-15 <span class="token number">18</span>:45:22  <span class="token punctuation">[</span>INFO<span class="token punctuation">]</span> Server started.</span>
<span class="line"><span class="token number">2024</span>-11-15 <span class="token number">18</span>:45:22  <span class="token punctuation">[</span>INFO<span class="token punctuation">]</span> Just-in-time model loading active.</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>You can use the endpoints to generate responses from the models. The endpoints are as follows:</p><ul><li><code>GET /v1/models</code>: This endpoint returns a list of the available models.</li><li><code>POST /v1/chat/completions</code>: This endpoint generates responses from the models using the chat format.Chat format is used for tasks such as chatbots, conversational AI, and language learning.</li><li><code>POST /v1/completions</code>: This endpoint generates responses from the models using the completion format. Completion format is used for tasks such as question answering, summarization, and text generation.</li><li><code>POST /v1/embeddings</code>: This endpoint generates embeddings from the models. Embeddings are used for tasks such as sentiment analysis, text classification, and language translation.</li></ul><h2 id="🧪-exercises" tabindex="-1"><a class="header-anchor" href="#🧪-exercises"><span>🧪 Exercises</span></a></h2><h2 id="📖-further-readings" tabindex="-1"><a class="header-anchor" href="#📖-further-readings"><span>📖 Further readings</span></a></h2></div><!--[--><!--]--></div><footer class="vp-page-meta"><div class="vp-meta-item edit-link"><a class="auto-link external-link label" href="https://github.com/worldline/learning-ai/edit/main/offline/README.md" aria-label="Edit this page" rel="noopener noreferrer" target="_blank"><!--[--><svg class="edit-icon" viewbox="0 0 1024 1024"><g fill="currentColor"><path d="M430.818 653.65a60.46 60.46 0 0 1-50.96-93.281l71.69-114.012 7.773-10.365L816.038 80.138A60.46 60.46 0 0 1 859.225 62a60.46 60.46 0 0 1 43.186 18.138l43.186 43.186a60.46 60.46 0 0 1 0 86.373L588.879 565.55l-8.637 8.637-117.466 68.234a60.46 60.46 0 0 1-31.958 11.229z"></path><path d="M728.802 962H252.891A190.883 190.883 0 0 1 62.008 771.98V296.934a190.883 190.883 0 0 1 190.883-192.61h267.754a60.46 60.46 0 0 1 0 120.92H252.891a69.962 69.962 0 0 0-69.098 69.099V771.98a69.962 69.962 0 0 0 69.098 69.098h475.911A69.962 69.962 0 0 0 797.9 771.98V503.363a60.46 60.46 0 1 1 120.922 0V771.98A190.883 190.883 0 0 1 728.802 962z"></path></g></svg><!--]-->Edit this page<!----></a></div><div class="vp-meta-item git-info"><div class="vp-meta-item last-updated"><span class="meta-item-label">Last Updated: </span><!----></div><div class="vp-meta-item contributors"><span class="meta-item-label">Contributors: </span><span class="meta-item-info"><!--[--><!--[--><span class="contributor" title="email: brah.gharbi@gmail.com">Brah</span><!----><!--]--><!--]--></span></div></div></footer><nav class="vp-page-nav" aria-label="page navigation"><a class="route-link auto-link prev" href="/learning-ai/llm/" aria-label="Online with Librechat"><div class="hint"><span class="arrow left"></span> Prev</div><div class="link"><span>Online with Librechat</span></div></a><a class="route-link auto-link next" href="/learning-ai/develop/" aria-label="Develop with AI"><div class="hint">Next <span class="arrow right"></span></div><div class="link"><span>Develop with AI</span></div></a></nav><!--[--><!--]--></main><!--]--></div><!--[--><!----><!--]--><!--]--></div>
    <script type="module" src="/learning-ai/assets/app-CpFOj0gG.js" defer></script>
  </body>
</html>
