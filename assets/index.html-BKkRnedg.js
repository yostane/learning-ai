import{_ as n,c as s,a,o as t}from"./app-CpFOj0gG.js";const o="/learning-ai/assets/lmstudio-B6PjFtYg.png",i={};function l(p,e){return t(),s("div",null,e[0]||(e[0]=[a('<h1 id="offline-with-lm-studio" tabindex="-1"><a class="header-anchor" href="#offline-with-lm-studio"><span>Offline with LM Studio</span></a></h1><div class="hint-container warning"><p class="hint-container-title">Disclamer</p><p>Be careful with offline prompting models downloaded from the internet. They can contain malicious code. And also the size of the model can be very large from few Gb to few Tb.</p></div><h2 id="definitions" tabindex="-1"><a class="header-anchor" href="#definitions"><span>Definitions</span></a></h2><p>If you don&#39;t want to use the online AI providers, you can use offline prompting. This technique involves using a local LLM to generate responses to prompts. It is useful for developers who want to use a local LLM for offline prompting or for those who want to experiment with different LLMs without relying on online providers.</p><p>LM Studio is a tool that allows developers to experiment with different LLMs without relying on online providers. It provides a user-friendly interface for selecting and configuring LLMs, as well as a chat interface for interacting with the LLMs. It also includes features for fine-tuning and deploying LLMs. This technique is useful for developers who want to experiment with different LLMs.</p><h2 id="installation" tabindex="-1"><a class="header-anchor" href="#installation"><span>Installation</span></a></h2><p><img src="'+o+`" alt="lmstudio_installation"></p><p>For installation, you can follow the instructions <a href="https://lmstudio.ai/docs/" target="_blank" rel="noopener noreferrer">here</a></p><h2 id="model-configuration" tabindex="-1"><a class="header-anchor" href="#model-configuration"><span>Model configuration</span></a></h2><p>You can configure the model you want to use in the settings tab. You can select the model you want to use and configure it according to your needs.</p><p><code>Context Length</code>: The context length is the number of tokens that will be used as context for the model. This is important because it determines how much information the model can use to generate a response. A longer context length will allow the model to generate more detailed and relevant responses, but it may also increase the computational cost of the model.</p><p><code>GPU Offload</code>: This option allows you to offload the model to a GPU if available. This can significantly speed up the generation process, especially for longer prompts or complex models.</p><p><code>CPU Threads</code>: This option allows you to specify the number of CPU threads to use for the model. This can be useful for controlling the computational resources used by the model.</p><p><code>Evaluation batch size</code>: This option allows you to specify the batch size for evaluation. This is important for evaluating the performance of the model and can affect the speed and accuracy of the generation process.</p><p><code>RoPE Frequency base</code>: This option allows you to specify the frequency base for RoPE (Range-based Output Embedding). This is important for controlling the output length of the model and can affect the quality of the generated responses.</p><p><code>RoPE Frequency scale</code>: This option allows you to specify the frequency scale for RoPE (Range-based Output Embedding). This is important for controlling the output length of the model and can affect the quality of the generated responses.</p><p><code>Keep model in memory</code>: This option allows you to keep the model in memory after the generation process is complete. This can be useful for generating multiple responses or for using the model for offline prompting.</p><p><code>Try mmap()</code> for faster loading: This option allows you to try using mmap() for faster loading of the model. This can be useful for loading large models or for generating responses quickly.</p><p><code>Seed</code>: This option allows you to specify a seed for the model. This can be useful for controlling the randomness of the generated responses.</p><p><code>Flash Attention</code>: This option allows you to enable flash attention for the model. This can be useful for generating more detailed and accurate responses, but it may also increase the computational cost of the model.</p><h2 id="enable-apis" tabindex="-1"><a class="header-anchor" href="#enable-apis"><span>enable APIs</span></a></h2><p>You can use the APIs to generate responses from the models. To enable the API server with LM Studio, you need to set the <code>API Server</code> option to <code>ON</code> in the settings tab. You can then use the API endpoints to generate responses from the models.</p><div class="language-bash line-numbers-mode" data-highlighter="prismjs" data-ext="sh" data-title="sh"><pre><code><span class="line"><span class="token number">2024</span>-11-15 <span class="token number">18</span>:45:22  <span class="token punctuation">[</span>INFO<span class="token punctuation">]</span> <span class="token punctuation">[</span>LM STUDIO SERVER<span class="token punctuation">]</span> Success<span class="token operator">!</span> HTTP server listening on port <span class="token number">1234</span></span>
<span class="line"><span class="token number">2024</span>-11-15 <span class="token number">18</span>:45:22  <span class="token punctuation">[</span>INFO<span class="token punctuation">]</span></span>
<span class="line"><span class="token number">2024</span>-11-15 <span class="token number">18</span>:45:22  <span class="token punctuation">[</span>INFO<span class="token punctuation">]</span> <span class="token punctuation">[</span>LM STUDIO SERVER<span class="token punctuation">]</span> Supported endpoints:</span>
<span class="line"><span class="token number">2024</span>-11-15 <span class="token number">18</span>:45:22  <span class="token punctuation">[</span>INFO<span class="token punctuation">]</span> <span class="token punctuation">[</span>LM STUDIO SERVER<span class="token punctuation">]</span> -<span class="token operator">&gt;</span>	GET  http://localhost:1234/v1/models</span>
<span class="line"><span class="token number">2024</span>-11-15 <span class="token number">18</span>:45:22  <span class="token punctuation">[</span>INFO<span class="token punctuation">]</span> <span class="token punctuation">[</span>LM STUDIO SERVER<span class="token punctuation">]</span> -<span class="token operator">&gt;</span>	POST http://localhost:1234/v1/chat/completions</span>
<span class="line"><span class="token number">2024</span>-11-15 <span class="token number">18</span>:45:22  <span class="token punctuation">[</span>INFO<span class="token punctuation">]</span> <span class="token punctuation">[</span>LM STUDIO SERVER<span class="token punctuation">]</span> -<span class="token operator">&gt;</span>	POST http://localhost:1234/v1/completions</span>
<span class="line"><span class="token number">2024</span>-11-15 <span class="token number">18</span>:45:22  <span class="token punctuation">[</span>INFO<span class="token punctuation">]</span> <span class="token punctuation">[</span>LM STUDIO SERVER<span class="token punctuation">]</span> -<span class="token operator">&gt;</span>	POST http://localhost:1234/v1/embeddings</span>
<span class="line"><span class="token number">2024</span>-11-15 <span class="token number">18</span>:45:22  <span class="token punctuation">[</span>INFO<span class="token punctuation">]</span></span>
<span class="line"><span class="token number">2024</span>-11-15 <span class="token number">18</span>:45:22  <span class="token punctuation">[</span>INFO<span class="token punctuation">]</span> <span class="token punctuation">[</span>LM STUDIO SERVER<span class="token punctuation">]</span> Logs are saved into /Users/ibrahim/.cache/lm-studio/server-logs</span>
<span class="line"><span class="token number">2024</span>-11-15 <span class="token number">18</span>:45:22  <span class="token punctuation">[</span>INFO<span class="token punctuation">]</span> Server started.</span>
<span class="line"><span class="token number">2024</span>-11-15 <span class="token number">18</span>:45:22  <span class="token punctuation">[</span>INFO<span class="token punctuation">]</span> Just-in-time model loading active.</span>
<span class="line"></span></code></pre><div class="line-numbers" aria-hidden="true" style="counter-reset:line-number 0;"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>You can use the endpoints to generate responses from the models. The endpoints are as follows:</p><ul><li><code>GET /v1/models</code>: This endpoint returns a list of the available models.</li><li><code>POST /v1/chat/completions</code>: This endpoint generates responses from the models using the chat format.Chat format is used for tasks such as chatbots, conversational AI, and language learning.</li><li><code>POST /v1/completions</code>: This endpoint generates responses from the models using the completion format. Completion format is used for tasks such as question answering, summarization, and text generation.</li><li><code>POST /v1/embeddings</code>: This endpoint generates embeddings from the models. Embeddings are used for tasks such as sentiment analysis, text classification, and language translation.</li></ul><h2 id="ðŸ§ª-exercises" tabindex="-1"><a class="header-anchor" href="#ðŸ§ª-exercises"><span>ðŸ§ª Exercises</span></a></h2><h2 id="ðŸ“–-further-readings" tabindex="-1"><a class="header-anchor" href="#ðŸ“–-further-readings"><span>ðŸ“– Further readings</span></a></h2>`,27)]))}const c=n(i,[["render",l],["__file","index.html.vue"]]),u=JSON.parse('{"path":"/offline/","title":"Offline with LM Studio","lang":"en-US","frontmatter":{},"headers":[{"level":2,"title":"Definitions","slug":"definitions","link":"#definitions","children":[]},{"level":2,"title":"Installation","slug":"installation","link":"#installation","children":[]},{"level":2,"title":"Model configuration","slug":"model-configuration","link":"#model-configuration","children":[]},{"level":2,"title":"enable APIs","slug":"enable-apis","link":"#enable-apis","children":[]},{"level":2,"title":"ðŸ§ª Exercises","slug":"ðŸ§ª-exercises","link":"#ðŸ§ª-exercises","children":[]},{"level":2,"title":"ðŸ“– Further readings","slug":"ðŸ“–-further-readings","link":"#ðŸ“–-further-readings","children":[]}],"git":{"updatedTime":1731692981000,"contributors":[{"name":"Brah","username":"Brah","email":"brah.gharbi@gmail.com","commits":1,"url":"https://github.com/Brah"}]},"filePathRelative":"offline/README.md"}');export{c as comp,u as data};
